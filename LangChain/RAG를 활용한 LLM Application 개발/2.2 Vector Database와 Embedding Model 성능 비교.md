# Vector Database와 Embedding Model 성능 비교
## 사용자가 원하는 정보
  - 사용자의 질문과 관련있는 데이터
  - 관련성 파익을 위해 vector를 활용함
    - 단어나 문장의 유사도를 파악해서 관련성을 측정
## Vector를 생성하는 방법
  - Embedding 모델을 활용해서 vector를 생성
  - 문장에서 비슷한 단어가 자주 붙어있는 것을 학습
    - 왕은 왕자의 아버지다
    - 여왕은 왕자의 어머니다
  - `왕자의`라는 단어 앞에 등장하는 `왕`과 `여왕`은 유사할 가능성이 높다

## Vector Database
- Embedding 모델을 활용해 생성된 vector를 저장
  - 단순 vector만 저장하면 안되고 metadata도 같이 저장
    - 이 항목이 상당히 중요
    - 문서의 이름, 페이지 번호 등등을 같이 저장
    - 잘못된 정보를 가져올 수도 있기 때문에, 어떤 문서에서 가져온지 출처를 같이 보여주는 것이 중요
      - LLM이 생성하는 답변의 퀄리티 상승(신뢰도 상승)
- Vector를 대상으로 유사도 검색 실시
  - 사용자의 질문과 가장 비슷한 문서를 가져오는 것 -> Retrieval
    - 문서 전체를 활용하면 속도도 느리고, 토큰수 초과로 답변 생성이 안될 수도 있음
    - 문서를 chunking, 나눠서 저장해야 함
  - 가져온 문서를 prompt를 통해 LLM에 제공 -> Augmented
  - LLM은 prompt를 활용해서 답변 생성 -> Generation
